{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data1 = np.random.normal(0, 1, 1000)\n",
    "data2 = np.random.normal(0, 1, size=(2, 1000))\n",
    "data3 = np.random.normal(0, 1, size=(4, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shannon Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shannon entropy H is given by the formula $H=-\\sum_{i}p_{i}\\log_{b}(p_{i})$\n",
    "where $p_{i}$ is the probability of character number $i$ appearing in the\n",
    "stream of characters of the message.\n",
    "\n",
    "Consider a simple digital circuit which has a two-bit input ($X$, $Y$) and a two-bit output ($X$ and $Y$, $X$ or $Y$). Assuming that the two input bits $X$ and $Y$ have mutually independent chances of $50%$ of being *HIGH*, then the input combinations $(0,0)$, $(0,1)$, $(1,0)$, and ($1,1)$ each have a 1/4 chance of occurring, so the circuit's Shannon entropy on the input side is $H(X,Y)=4{\\Big (}-{1 \\over 4}\\log _{2}{1 \\over 4}{\\Big )}=2$ Then the possible output combinations are (0,0), (0,1) and (1,1) with respective chances of 1/4, 1/2, and 1/4 of occurring, so the circuit's Shannon entropy on the output side is $H(X{\\text{ and }}Y,X{\\text{ or }}Y)=2{\\Big (}-{1 \\over 4}\\log _{2}{1 \\over 4}{\\Big )}-{1 \\over 2}\\log _{2}{1 \\over 2}=1+{1 \\over 2}=1{1 \\over 2}$, so the circuit reduces (or \"orders\") the information going through it by half a\n",
    "bit of Shannon entropy due to its logical irreversibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (1000,)\n",
      "Entropy: 3.4263432580844695\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gcpds.entropies import Shannon\n",
    "\n",
    "ent = Shannon(data1)\n",
    "print(f\"Input data shape: {data1.shape}\")\n",
    "print(f\"Entropy: {ent}\", end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0154281530433003"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Shannon(data1, base=10)  # Default base is 2\n",
    "Shannon(data1, bins=12)  # Default bins value used to calculate the distribution is 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.348718451910984"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Shannon(data2, conditional=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint entropy\n",
    "\n",
    "For 2 variables:  \n",
    "${\\displaystyle \\mathrm {H} (X,Y)=-\\sum _{x\\in {\\mathcal {X}}}\\sum _{y\\in {\\mathcal {Y}}}P(x,y)\\log _{2}[P(x,y)]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (2, 1000)\n",
      "Entropy: 6.454989877719003\n"
     ]
    }
   ],
   "source": [
    "ent = Shannon(data2)\n",
    "\n",
    "print(f\"Input data shape: {data2.shape}\")\n",
    "print(f\"Entropy: {ent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more than two random variables ${\\displaystyle X_{1},...,X_{n}} X_{1},...,X_{n}$ this expands to  \n",
    "${\\displaystyle \\mathrm {H} (X_{1},...,X_{n})=-\\sum _{x_{1}\\in {\\mathcal {X}}_{1}}...\\sum _{x_{n}\\in {\\mathcal {X}}_{n}}P(x_{1},...,x_{n})\\log _{2}[P(x_{1},...,x_{n})]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (4, 1000)\n",
      "Entropy: 9.801254959649105\n"
     ]
    }
   ],
   "source": [
    "ent = Shannon(data3)\n",
    "\n",
    "print(f\"Input data shape: {data3.shape}\")\n",
    "print(f\"Entropy: {ent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional entropy\n",
    "\n",
    "Joint entropy is used in the definition of conditional entropy  \n",
    "${\\displaystyle \\mathrm {H} (X|Y)=\\mathrm {H} (X,Y)-\\mathrm {H} (Y)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (4, 1000)\n",
      "Entropy: 6.332143741478394\n"
     ]
    }
   ],
   "source": [
    "ent = Shannon(data3, conditional=0)  # `conditional` is an index of the input array\n",
    "\n",
    "print(f\"Input data shape: {data3.shape}\")\n",
    "print(f\"Entropy: {ent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### References\n",
    "\n",
    "  * Thomas M. Cover; Joy A. Thomas. Elements of Information Theory. Hoboken, New Jersey: Wiley. ISBN 0-471-24195-4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
